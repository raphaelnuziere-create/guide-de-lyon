// Script pour initialiser les sources de scraping
import { createClient } from '@supabase/supabase-js';

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL!;
const supabaseKey = process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!;

const supabase = createClient(supabaseUrl, supabaseKey);

async function initSources() {
  console.log('ğŸš€ Initialisation des sources de scraping\n');
  
  const sources = [
    {
      name: 'Le ProgrÃ¨s Lyon RSS',
      url: 'https://www.leprogres.fr/edition-lyon-villeurbanne/rss',
      type: 'rss',
      selectors: {
        title: 'title',
        description: 'description',
        link: 'link',
        pubDate: 'pubDate',
        image: 'enclosure[type="image/jpeg"]'
      },
      frequency_minutes: 60,
      is_active: true
    },
    {
      name: 'Lyon Capitale RSS',
      url: 'https://www.lyoncapitale.fr/feed/',
      type: 'rss',
      selectors: {
        title: 'title',
        description: 'description',
        link: 'link',
        pubDate: 'pubDate'
      },
      frequency_minutes: 90,
      is_active: true
    },
    {
      name: 'Tribune de Lyon RSS',
      url: 'https://tribunedelyon.fr/feed/',
      type: 'rss',
      selectors: {
        title: 'title',
        description: 'description',
        link: 'link',
        pubDate: 'pubDate'
      },
      frequency_minutes: 120,
      is_active: true
    }
  ];
  
  console.log(`ğŸ“ Insertion de ${sources.length} sources...`);
  
  for (const source of sources) {
    try {
      // VÃ©rifier si la source existe dÃ©jÃ 
      const { data: existing } = await supabase
        .from('scraping_sources')
        .select('id')
        .eq('url', source.url)
        .single();
      
      if (existing) {
        console.log(`âš ï¸  ${source.name}: Existe dÃ©jÃ `);
        continue;
      }
      
      // InsÃ©rer la nouvelle source
      const { error } = await supabase
        .from('scraping_sources')
        .insert(source);
      
      if (error) {
        console.log(`âŒ ${source.name}: Erreur - ${error.message}`);
      } else {
        console.log(`âœ… ${source.name}: AjoutÃ©e`);
      }
    } catch (err) {
      console.log(`âŒ ${source.name}: Erreur inattendue`);
    }
  }
  
  // VÃ©rifier le rÃ©sultat
  console.log('\nğŸ“Š RÃ©sumÃ©:');
  const { data: allSources, error } = await supabase
    .from('scraping_sources')
    .select('*');
  
  if (!error && allSources) {
    console.log(`Total des sources: ${allSources.length}`);
    console.log(`Sources actives: ${allSources.filter(s => s.is_active).length}`);
    
    console.log('\nğŸ“° Sources configurÃ©es:');
    allSources.forEach(s => {
      console.log(`  ${s.is_active ? 'âœ…' : 'âŒ'} ${s.name} - ${s.type} - ${s.frequency_minutes}min`);
    });
  }
  
  console.log('\nğŸ¯ Prochaines Ã©tapes:');
  console.log('1. DÃ©marrez le serveur: npm run dev');
  console.log('2. DÃ©clenchez le scraping: npx tsx scripts/trigger-scraping.ts');
  console.log('3. Consultez les articles: http://localhost:3000/actualites');
}

initSources()
  .then(() => {
    console.log('\nâœ… Initialisation terminÃ©e');
    process.exit(0);
  })
  .catch((error) => {
    console.error('âŒ Erreur:', error);
    process.exit(1);
  });