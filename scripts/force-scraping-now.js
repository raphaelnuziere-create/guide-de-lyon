#!/usr/bin/env node

/**
 * Script pour forcer le scraping MAINTENANT apr√®s nettoyage
 */

console.log('üöÄ LANCEMENT FORC√â DU SCRAPING\n');
console.log('=' .repeat(50));

async function forceScraping() {
  try {
    // Test LOCAL d'abord
    console.log('üì° Test en LOCAL (http://localhost:3000)...\n');
    
    const localUrl = 'http://localhost:3000/api/scraping/run-full';
    
    console.log('‚è≥ Scraping en cours (30-60 secondes)...\n');
    
    const response = await fetch(localUrl, {
      method: 'GET',
      headers: {
        'Content-Type': 'application/json',
      },
      // Timeout de 2 minutes
      signal: AbortSignal.timeout(120000)
    });
    
    const data = await response.json();
    
    if (response.ok) {
      console.log('‚úÖ SCRAPING R√âUSSI !\n');
      console.log('üìä R√©sultat:');
      console.log(JSON.stringify(data, null, 2));
      
      console.log('\n' + '=' .repeat(50));
      console.log('‚ú® V√âRIFICATIONS :');
      console.log('1. LOCAL : http://localhost:3000/actualites');
      console.log('2. PROD : https://www.guide-de-lyon.fr/actualites');
      console.log('3. SUPABASE : V√©rifier scraped_articles');
      console.log('\n‚è∞ Le cron job reprendra automatiquement toutes les heures');
      
    } else {
      console.error('‚ùå ERREUR:', data);
      
      if (data.error?.includes('OpenAI')) {
        console.log('\n‚ö†Ô∏è Probl√®me avec OpenAI - V√©rifiez la cl√© API');
      }
    }
  } catch (error) {
    if (error.name === 'AbortError') {
      console.error('‚ùå Timeout - Le scraping prend trop de temps');
    } else {
      console.error('‚ùå Erreur:', error.message);
    }
    
    console.log('\nüí° V√©rifiez que :');
    console.log('1. Le serveur est lanc√© (npm run dev)');
    console.log('2. Les tables sont nettoy√©es dans Supabase');
    console.log('3. La source 20 Minutes est configur√©e');
  }
}

// Lancer
forceScraping();